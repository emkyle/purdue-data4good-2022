{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Captioning Example (Purdue Krannert-Krenicki-Microsoft-INFORMS Analytics Competition)","metadata":{"id":"xT0ciNrUgnop"}},{"cell_type":"markdown","source":"## Dependencies and imports","metadata":{"id":"kfI4-lw2f4Rg"}},{"cell_type":"markdown","source":"These are dependencies that needed to be installed within the environment that this notebook was run. Depending on where you run the notebook, you may need to install other dependencies. See the below list of imports for details on what packages are used. Additionally, here is some version information for third party libraries that are known to work with this example:\n\n- Tensorflow 2.9.2\n- Datasets 2.6.1\n- Numpy 1.21.6","metadata":{"id":"089jK0pegTb5"}},{"cell_type":"code","source":"#! pip install datasets tqdm","metadata":{"id":"iuzUalGEjNke","outputId":"dd048b6f-6562-49a8-dfc2-d9222dec0e03","execution":{"iopub.status.busy":"2022-11-14T23:18:15.869877Z","iopub.execute_input":"2022-11-14T23:18:15.870187Z","iopub.status.idle":"2022-11-14T23:18:15.888892Z","shell.execute_reply.started":"2022-11-14T23:18:15.870114Z","shell.execute_reply":"2022-11-14T23:18:15.888180Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import collections; import random; import os; import time; import json; from PIL import Image;\nimport io; import urllib; import uuid; from concurrent.futures import ThreadPoolExecutor;\nfrom functools import partial;\n\nimport numpy as np; from tqdm import tqdm; from datasets import load_dataset;\nfrom datasets.utils.file_utils import get_datasets_user_agent; import matplotlib.pyplot as plt;\nimport tensorflow as tf; from datasets import load_dataset;\n#hello","metadata":{"id":"ZXvljgc-jACE","execution":{"iopub.status.busy":"2022-11-14T23:18:29.710514Z","iopub.execute_input":"2022-11-14T23:18:29.711032Z","iopub.status.idle":"2022-11-14T23:18:29.717230Z","shell.execute_reply.started":"2022-11-14T23:18:29.711007Z","shell.execute_reply":"2022-11-14T23:18:29.716367Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Set Random Seeds","metadata":{"id":"N5TzlNf3dWMX"}},{"cell_type":"markdown","source":"By setting random seed values, you can make your code a bit more reproducible between runs. This will help you as you experiment to see performance boosts due to methodology vs. differences due to initial conditions.","metadata":{"id":"yMZmAIGBhMCy"}},{"cell_type":"code","source":"# Seed value\nseed_value= 1022\n\n# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set the `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set the `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\nimport tensorflow as tf\ntf.random.set_seed(seed_value)\n\n# 5. Configure a new global `tensorflow` session\nfrom keras import backend as K\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","metadata":{"id":"E0IGjMK_dZIZ","execution":{"iopub.status.busy":"2022-11-14T23:18:47.444234Z","iopub.execute_input":"2022-11-14T23:18:47.444612Z","iopub.status.idle":"2022-11-14T23:18:48.667560Z","shell.execute_reply.started":"2022-11-14T23:18:47.444584Z","shell.execute_reply":"2022-11-14T23:18:48.666258Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Data and pre-processing","metadata":{"id":"Rc3smN2MjTy9"}},{"cell_type":"markdown","source":"Recently, a team at SIL processed the data from the [Bloom Library](https://bloomlibrary.org) in \"AI-ready\" datasets for language modeling, image captioning, speech recognition, and visual storytelling tasks. In particular, the [bloom-captioning](https://huggingface.co/datasets/sil-ai/bloom-captioning) dataset enables developers and researchers to train and/or test image captioning models in 351 languages including Hausa, Kyrgyz, and Thai. This data can be accessed on Hugging Face and/or via the \"datasets\" Python library from Hugging Face.\n\nThe bloom-captioning dataset includes training, test, and validation splits for many languages, but this competition will only focus on Hausa, Kyrgyz, and Thai. These three languages represent diverse writing system scripts and geographies. The test split of the data does not include the gold standard captions paired with each image. Your job will be to generate the captions for the test split images and upload them to Kaggle. An automatic evaluation will be run on Kaggle to compare your generated captions with the gold standard captions. This evaluation will be used to rank your submission.\n\n**Note** - In order to use this dataset, you will need to have a free account on Hugging Face and agree to the terms of use for the dataset. You can do this by:\n\n1. Creating your HF account (if you don't yet have one)\n2. Navigating to your HF account and [create a new \"access token\"](https://huggingface.co/settings/tokens)\n2. Navigating to the [bloom-captioning](https://huggingface.co/datasets/sil-ai/bloom-captioning) dataset page\n3. Agreeing to the terms of use at the top of the dataset card\n4. Downloading and processing the dataset as illustrated below","metadata":{"id":"oRO00ulbhlBx"}},{"cell_type":"markdown","source":"### Fetch the dataset from HF","metadata":{"id":"kEdDl1KVjx6K"}},{"cell_type":"markdown","source":"Because HF users need to agree to the terms of use of the dataset prior to usage, you need to login to HF as shown below:","metadata":{"id":"CbGgM1tTiW43"}},{"cell_type":"code","source":"#! huggingface-cli login\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n#Here's my HF Read Key for this notebook to use\n# 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","metadata":{"id":"dUJlz1PmjEgr","outputId":"ef041b67-2cc8-4359-ae57-7aa81b35d262","execution":{"iopub.status.busy":"2022-11-12T13:57:40.225604Z","iopub.execute_input":"2022-11-12T13:57:40.226858Z","iopub.status.idle":"2022-11-12T13:57:40.277091Z","shell.execute_reply.started":"2022-11-12T13:57:40.226802Z","shell.execute_reply":"2022-11-12T13:57:40.275958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the relevant ISO code for the language you want to work with.\niso639_3_letter_code = \"hau\"\n#iso639_3_letter_code = \"tha\"\n#iso639_3_letter_code = \"kir\"\n\n# Download the language specific dataset from HF.\ndataset = load_dataset(\"sil-ai/bloom-captioning\", iso639_3_letter_code, \n                       use_auth_token=True, download_mode='force_redownload')","metadata":{"id":"RocNxtYHjXnr","outputId":"65cf6b14-4133-4059-c347-606ed62e5d93","execution":{"iopub.status.busy":"2022-11-12T13:57:53.091599Z","iopub.execute_input":"2022-11-12T13:57:53.092500Z","iopub.status.idle":"2022-11-12T13:58:24.640806Z","shell.execute_reply.started":"2022-11-12T13:57:53.092451Z","shell.execute_reply":"2022-11-12T13:58:24.639498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See what is included in the dataset object.\ndataset","metadata":{"id":"VA0T9Nb9KjYe","outputId":"9769f1ed-e244-40c8-bb2b-4ac17b06f58c","execution":{"iopub.status.busy":"2022-11-12T13:58:39.569363Z","iopub.execute_input":"2022-11-12T13:58:39.569760Z","iopub.status.idle":"2022-11-12T13:58:39.579608Z","shell.execute_reply.started":"2022-11-12T13:58:39.569728Z","shell.execute_reply":"2022-11-12T13:58:39.578459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many samples (image-caption pairs) are included in our training set.\nlen(dataset['train'])","metadata":{"id":"6g7n52cGZo7G","outputId":"d57cd026-b028-4033-c584-07420e1cffb5","execution":{"iopub.status.busy":"2022-11-12T13:59:06.721976Z","iopub.execute_input":"2022-11-12T13:59:06.722415Z","iopub.status.idle":"2022-11-12T13:59:06.728766Z","shell.execute_reply.started":"2022-11-12T13:59:06.722366Z","shell.execute_reply":"2022-11-12T13:59:06.727681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check one of the training samples.\ndataset['train'][0]","metadata":{"id":"IgIzElWKI97p","outputId":"429afdf2-d387-42db-fdef-6e925d3f78fe","execution":{"iopub.status.busy":"2022-11-12T13:59:10.093406Z","iopub.execute_input":"2022-11-12T13:59:10.093835Z","iopub.status.idle":"2022-11-12T13:59:10.102487Z","shell.execute_reply.started":"2022-11-12T13:59:10.093797Z","shell.execute_reply":"2022-11-12T13:59:10.101730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check one of the test samples. Notice the hidden caption.\ndataset['test'][0]","metadata":{"id":"A_jul7FWJCW5","outputId":"0d432718-2888-4a22-9aa5-11ab1f66989e","execution":{"iopub.status.busy":"2022-11-12T14:00:11.031303Z","iopub.execute_input":"2022-11-12T14:00:11.031753Z","iopub.status.idle":"2022-11-12T14:00:11.039759Z","shell.execute_reply.started":"2022-11-12T14:00:11.031717Z","shell.execute_reply":"2022-11-12T14:00:11.038632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fetch the images","metadata":{"id":"jREU72DMjuua"}},{"cell_type":"markdown","source":"The actual images are NOT downloaded when you download the Hugging Face dataset. The HF dataset merely includes the public links to the images. As such, you need to download the actual image files. ","metadata":{"id":"Cy6CMOW1j6dm"}},{"cell_type":"code","source":"! rm -rf images\n! mkdir images\n\nUSER_AGENT = get_datasets_user_agent()\n\ndef fetch_single_image(image_url, timeout=None, retries=0):\n    request = urllib.request.Request(\n        image_url,\n        data=None,\n        headers={\"user-agent\": USER_AGENT},\n    )\n    with urllib.request.urlopen(request, timeout=timeout) as req:\n        if 'png' in image_url:\n          png = Image.open(io.BytesIO(req.read())).convert('RGBA')\n          png.load() # required for png.split()\n          background = Image.new(\"RGB\", png.size, (255, 255, 255))\n          background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n          image_id = str(uuid.uuid4())\n          image_path = \"images/\" + image_id + \".jpg\"\n          background.save(image_path, 'JPEG', quality=80)\n        else:\n          image = Image.open(io.BytesIO(req.read()))\n          image_id = str(uuid.uuid4())\n          image_path = \"images/\" + image_id + \".jpg\"\n          image.save(image_path)\n    return image_path\n\ndef fetch_images(batch, num_threads, timeout=None, retries=3):\n    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        batch[\"image_path\"] = list(executor.map(fetch_single_image_with_args, batch[\"image_url\"]))\n    return batch\n\nnum_threads = 20\ndataset = dataset.map(fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": num_threads})","metadata":{"id":"F4E1bg5gjkrl","outputId":"8526fca3-3489-4fa6-db4a-a2182439385e","execution":{"iopub.status.busy":"2022-11-12T14:00:41.307339Z","iopub.execute_input":"2022-11-12T14:00:41.308159Z","iopub.status.idle":"2022-11-12T14:03:12.968069Z","shell.execute_reply.started":"2022-11-12T14:00:41.308118Z","shell.execute_reply":"2022-11-12T14:03:12.967027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notice that we now have a new field in our dataset object called \"image_path\"\ndataset['train']","metadata":{"id":"oKv-iFodIb32","outputId":"0ae15a17-1d17-4692-b94a-cb4d40440c05","execution":{"iopub.status.busy":"2022-11-12T14:03:56.276694Z","iopub.execute_input":"2022-11-12T14:03:56.277088Z","iopub.status.idle":"2022-11-12T14:03:56.285482Z","shell.execute_reply.started":"2022-11-12T14:03:56.277053Z","shell.execute_reply":"2022-11-12T14:03:56.284284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare images features with a pre-trained InceptionV3 model","metadata":{"id":"6SJP7DCPNYQe"}},{"cell_type":"markdown","source":"This model assumes that the pretrained image encoder is sufficient, and just focuses on building the text decoder. We use the InceptionV3 model for the pretrained image encoder. Read more about InceptionV3 [here](https://keras.io/api/applications/inceptionv3/).","metadata":{"id":"CD3HiQswmpOA"}},{"cell_type":"markdown","source":"### Retrieve the InceptionV3 model","metadata":{"id":"ybyKTOfNm3_v"}},{"cell_type":"code","source":"image_model = tf.keras.applications.InceptionV3(include_top=False,\n                                                weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","metadata":{"id":"m4ZhUTsxNaGM","outputId":"25a04853-652e-4918-ceec-21c954709590","execution":{"iopub.status.busy":"2022-11-12T14:04:17.404725Z","iopub.execute_input":"2022-11-12T14:04:17.405136Z","iopub.status.idle":"2022-11-12T14:04:25.309854Z","shell.execute_reply.started":"2022-11-12T14:04:17.405101Z","shell.execute_reply":"2022-11-12T14:04:25.308666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cache image features","metadata":{"id":"9YALesnjOIVl"}},{"cell_type":"markdown","source":"Here we will use the downloaded InceptionV3 model to calculate the encoded features of our images. We will cache these features locally to utilize them during training. ","metadata":{"id":"2tvPGx__nfBy"}},{"cell_type":"code","source":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.keras.layers.Resizing(299, 299)(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path","metadata":{"id":"fDrwRbjhOeQd","execution":{"iopub.status.busy":"2022-11-12T14:04:36.749458Z","iopub.execute_input":"2022-11-12T14:04:36.749844Z","iopub.status.idle":"2022-11-12T14:04:36.756205Z","shell.execute_reply.started":"2022-11-12T14:04:36.749812Z","shell.execute_reply":"2022-11-12T14:04:36.754914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get unique train images\nencode_train = sorted(set(dataset['train']['image_path']))\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(\n  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n\nfor img, path in tqdm(image_dataset):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    np.save(path_of_feature, bf.numpy())\n\n# Get unique test images\nencode_test = sorted(set(dataset['test']['image_path']))\n\n# Feel free to change batch_size according to your system configuration\nimage_dataset_test = tf.data.Dataset.from_tensor_slices(encode_test)\nimage_dataset_test = image_dataset_test.map(\n  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n\nfor img, path in tqdm(image_dataset_test):\n  batch_features = image_features_extract_model(img)\n  batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n\n  for bf, p in zip(batch_features, path):\n    path_of_feature = p.numpy().decode(\"utf-8\")\n    np.save(path_of_feature, bf.numpy())","metadata":{"id":"5SsJq0ZcOKWy","outputId":"eaf64a42-2177-4099-ed31-059c16abe327","execution":{"iopub.status.busy":"2022-11-12T14:04:40.504531Z","iopub.execute_input":"2022-11-12T14:04:40.505079Z","iopub.status.idle":"2022-11-12T14:12:10.456807Z","shell.execute_reply.started":"2022-11-12T14:04:40.505021Z","shell.execute_reply":"2022-11-12T14:12:10.455967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare captions","metadata":{"id":"ChClx3a5Zoi5"}},{"cell_type":"code","source":"# Add some special tokens and clean up new line characters.\ntrain_captions = [f\"<start> {x} <end>\" for x in dataset['train']['caption']]\ntrain_captions = [x.replace('\\n', ' ') for x in train_captions]\ntest_captions = [f\"<start> {x} <end>\" for x in dataset['test']['caption']]\ntest_captions = [x.replace('\\n', ' ') for x in test_captions]","metadata":{"id":"78tFrte9n0li","execution":{"iopub.status.busy":"2022-11-12T14:13:22.292072Z","iopub.execute_input":"2022-11-12T14:13:22.292495Z","iopub.status.idle":"2022-11-12T14:13:22.303560Z","shell.execute_reply.started":"2022-11-12T14:13:22.292459Z","shell.execute_reply":"2022-11-12T14:13:22.302447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n\n# We will override the default standardization of TextVectorization to preserve\n# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\ndef standardize(inputs):\n  inputs = tf.strings.lower(inputs)\n  return tf.strings.regex_replace(inputs,\n                                  r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n\n# Max word count for a caption.\nmax_length = 50\n# Use the top 5000 words for a vocabulary.\nvocabulary_size = 5000\ntokenizer = tf.keras.layers.TextVectorization(\n    max_tokens=vocabulary_size,\n    standardize=standardize,\n    output_sequence_length=max_length)\n# Learn the vocabulary from the caption data.\ntokenizer.adapt(caption_dataset)\n\n# Create the tokenized vectors\ncap_vector = caption_dataset.map(lambda x: tokenizer(x))\n\n# Create mappings for words to indices and indicies to words.\nword_to_index = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary())\nindex_to_word = tf.keras.layers.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary(),\n    invert=True)\n\ncaption_dataset_test = tf.data.Dataset.from_tensor_slices(test_captions)\ncap_vector_test = caption_dataset_test.map(lambda x: tokenizer(x))","metadata":{"id":"JKKFjdCPQVKo","execution":{"iopub.status.busy":"2022-11-12T14:13:25.276219Z","iopub.execute_input":"2022-11-12T14:13:25.276652Z","iopub.status.idle":"2022-11-12T14:13:27.028639Z","shell.execute_reply.started":"2022-11-12T14:13:25.276610Z","shell.execute_reply":"2022-11-12T14:13:27.027495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## More data prep for training","metadata":{"id":"h-Sjl0cvaTuO"}},{"cell_type":"code","source":"# Create some mas between images, vectors, and captions\nimg_to_cap_vector = collections.defaultdict(list)\nfor img, cap in zip(dataset['train']['image_path'], cap_vector):\n  img_to_cap_vector[img].append(cap)\n\nimg_name_train = []\ncap_train = []\nfor imgt in list(img_to_cap_vector.keys()):\n  capt_len = len(img_to_cap_vector[imgt])\n  img_name_train.extend([imgt] * capt_len)\n  cap_train.extend(img_to_cap_vector[imgt])\n\nimg_to_cap_vector_test = collections.defaultdict(list)\nfor img, cap in zip(dataset['test']['image_path'], cap_vector_test):\n  img_to_cap_vector_test[img].append(cap)\n\nimg_name_test = []\ncap_test = []\nfor imgv in list(img_to_cap_vector_test.keys()):\n  capv_len = len(img_to_cap_vector_test[imgv])\n  img_name_test.extend([imgv] * capv_len)\n  cap_test.extend(img_to_cap_vector_test[imgv])","metadata":{"id":"SjhZpyC3aQ64","execution":{"iopub.status.busy":"2022-11-12T14:13:39.783170Z","iopub.execute_input":"2022-11-12T14:13:39.784132Z","iopub.status.idle":"2022-11-12T14:13:40.540763Z","shell.execute_reply.started":"2022-11-12T14:13:39.784078Z","shell.execute_reply":"2022-11-12T14:13:40.539448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Double check the lengths of our new arrays.\nlen(img_name_train), len(cap_train)","metadata":{"id":"OmKOjSSSaUfu","outputId":"89172caf-a9c6-41cf-e1a9-6972c378fc87","execution":{"iopub.status.busy":"2022-11-12T14:13:44.786656Z","iopub.execute_input":"2022-11-12T14:13:44.787036Z","iopub.status.idle":"2022-11-12T14:13:44.794340Z","shell.execute_reply.started":"2022-11-12T14:13:44.787006Z","shell.execute_reply":"2022-11-12T14:13:44.792979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feel free to change these parameters according to your system's configuration\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 256\nunits = 512\nnum_steps = len(img_name_train) // BATCH_SIZE\n\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048\nattention_features_shape = 64","metadata":{"id":"_V7xQdp2aY5r","execution":{"iopub.status.busy":"2022-11-12T14:13:53.260013Z","iopub.execute_input":"2022-11-12T14:13:53.260983Z","iopub.status.idle":"2022-11-12T14:13:53.266672Z","shell.execute_reply.started":"2022-11-12T14:13:53.260944Z","shell.execute_reply":"2022-11-12T14:13:53.265567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the numpy files\ndef map_func(img_name, cap):\n  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n  return img_tensor, cap","metadata":{"id":"hYFY2qUyarMB","execution":{"iopub.status.busy":"2022-11-12T14:13:57.844245Z","iopub.execute_input":"2022-11-12T14:13:57.844629Z","iopub.status.idle":"2022-11-12T14:13:57.848880Z","shell.execute_reply.started":"2022-11-12T14:13:57.844597Z","shell.execute_reply":"2022-11-12T14:13:57.848052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_tf = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\n# Use map to load the numpy files in parallel\ndataset_tf = dataset_tf.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int64]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ndataset_tf = dataset_tf.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset_tf = dataset_tf.prefetch(buffer_size=tf.data.AUTOTUNE)","metadata":{"id":"YJe-wn8xasdd","execution":{"iopub.status.busy":"2022-11-12T14:14:01.022348Z","iopub.execute_input":"2022-11-12T14:14:01.022744Z","iopub.status.idle":"2022-11-12T14:14:01.099439Z","shell.execute_reply.started":"2022-11-12T14:14:01.022712Z","shell.execute_reply":"2022-11-12T14:14:01.098588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define our image captioning model","metadata":{"id":"uQOfL_jsawMp"}},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # attention_hidden_layer shape == (batch_size, 64, units)\n    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n                                         self.W2(hidden_with_time_axis)))\n\n    # score shape == (batch_size, 64, 1)\n    # This gives you an unnormalized score for each image feature.\n    score = self.V(attention_hidden_layer)\n\n    # attention_weights shape == (batch_size, 64, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights\n","metadata":{"id":"EmIemwgNatoq","execution":{"iopub.status.busy":"2022-11-12T14:14:08.479738Z","iopub.execute_input":"2022-11-12T14:14:08.480597Z","iopub.status.idle":"2022-11-12T14:14:08.488061Z","shell.execute_reply.started":"2022-11-12T14:14:08.480563Z","shell.execute_reply":"2022-11-12T14:14:08.487119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 64, embedding_dim)\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x\n","metadata":{"id":"GyxXsF3_axFb","execution":{"iopub.status.busy":"2022-11-12T14:14:12.483309Z","iopub.execute_input":"2022-11-12T14:14:12.483703Z","iopub.status.idle":"2022-11-12T14:14:12.491729Z","shell.execute_reply.started":"2022-11-12T14:14:12.483671Z","shell.execute_reply":"2022-11-12T14:14:12.490376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))\n","metadata":{"id":"88T53JesazIr","execution":{"iopub.status.busy":"2022-11-12T14:14:16.004996Z","iopub.execute_input":"2022-11-12T14:14:16.005732Z","iopub.status.idle":"2022-11-12T14:14:16.015621Z","shell.execute_reply.started":"2022-11-12T14:14:16.005696Z","shell.execute_reply":"2022-11-12T14:14:16.014611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the encoder and decoder\nencoder = CNN_Encoder(embedding_dim)\ndecoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())","metadata":{"id":"5wKm0qbPa0wq","execution":{"iopub.status.busy":"2022-11-12T14:14:20.676146Z","iopub.execute_input":"2022-11-12T14:14:20.676802Z","iopub.status.idle":"2022-11-12T14:14:20.696221Z","shell.execute_reply.started":"2022-11-12T14:14:20.676768Z","shell.execute_reply":"2022-11-12T14:14:20.695079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training config.\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\n# Loss function to use during training.\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","metadata":{"id":"1-L6W_p9a2Kx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup and execute training","metadata":{"id":"ZHqzia1ea37r"}},{"cell_type":"code","source":"# Make sure we save checkpoints during training\ncheckpoint_path = \"./checkpoints/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer=optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)","metadata":{"id":"ggMkCTKEa3Z9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n  \n  # restoring the latest checkpoint in checkpoint_path\n  ckpt.restore(ckpt_manager.latest_checkpoint)","metadata":{"id":"sRatak4Na6gd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding this in a separate cell because if you run the training cell\n# many times, the loss_plot array will be reset\nloss_plot = []","metadata":{"id":"M0828wbXa8d_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n\n      for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n          loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n          dec_input = tf.expand_dims(target[:, i], 1)\n\n  total_loss = (loss / int(target.shape[1]))\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n  gradients = tape.gradient(loss, trainable_variables)\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, total_loss","metadata":{"id":"MPd14RVGa_IJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adjust this depending on how long you want to train\nEPOCHS = 5\n\n# Train our model!\nfor epoch in range(start_epoch, EPOCHS):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(dataset_tf):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n\n        if batch % 100 == 0:\n            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n    # storing the epoch end loss value to plot later\n    loss_plot.append(total_loss / num_steps)\n\n    if epoch % 5 == 0:\n      ckpt_manager.save()\n\n    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')","metadata":{"id":"Yw_aGJpsbBdp","outputId":"bf095fd3-bf17-43ff-d89b-7a4262e5b901"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize our loss\nplt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"id":"M8zS5bsUbDBC","outputId":"7a89c64f-bf0c-4602-dcf6-120da4eac519"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict a caption for the test set","metadata":{"id":"-1a3AwSMMsaM"}},{"cell_type":"code","source":"def predict(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n                                                 -1,\n                                                 img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input,\n                                                         features,\n                                                         hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n        result.append(predicted_word)\n\n        if predicted_word == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","metadata":{"id":"tQcF5FuPMvHf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for i in range(len_result):\n        temp_att = np.resize(attention_plot[i], (8, 8))\n        grid_size = max(int(np.ceil(len_result/2)), 2)\n        ax = fig.add_subplot(grid_size, grid_size, i+1)\n        ax.set_title(result[i])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"6VGYuJ-oM7iP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict a caption for a random test image\nrid = np.random.randint(0, len(img_name_test))\nimage = img_name_test[rid]\nresult, attention_plot = predict(image)\npredicted_caption = ' '.join(result).replace(' <end>', '')\nprint('Prediction Caption:', predicted_caption)\n\n# Display image\nfrom IPython.display import Image as im\nim(filename=image) ","metadata":{"id":"m_kNYfd1M-mx","outputId":"cfe8d738-a4bb-490c-8e78-7838d748e3e1"},"execution_count":null,"outputs":[]}]}