{"cells":[{"cell_type":"markdown","metadata":{"id":"xT0ciNrUgnop"},"source":["# Image Captioning Example (Purdue Krannert-Krenicki-Microsoft-INFORMS Analytics Competition)"]},{"cell_type":"markdown","metadata":{"id":"kfI4-lw2f4Rg"},"source":["## Dependencies and imports"]},{"cell_type":"markdown","metadata":{"id":"089jK0pegTb5"},"source":["These are dependencies that needed to be installed within the environment that this notebook was run. Depending on where you run the notebook, you may need to install other dependencies. See the below list of imports for details on what packages are used. Additionally, here is some version information for third party libraries that are known to work with this example:\n","\n","- Tensorflow 2.9.2\n","- Datasets 2.6.1\n","- Numpy 1.21.6"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T23:18:15.870187Z","iopub.status.busy":"2022-11-14T23:18:15.869877Z","iopub.status.idle":"2022-11-14T23:18:15.888892Z","shell.execute_reply":"2022-11-14T23:18:15.888180Z","shell.execute_reply.started":"2022-11-14T23:18:15.870114Z"},"id":"iuzUalGEjNke","outputId":"dd048b6f-6562-49a8-dfc2-d9222dec0e03","trusted":true},"outputs":[],"source":["# pip install tensorflow"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T23:18:29.711032Z","iopub.status.busy":"2022-11-14T23:18:29.710514Z","iopub.status.idle":"2022-11-14T23:18:29.717230Z","shell.execute_reply":"2022-11-14T23:18:29.716367Z","shell.execute_reply.started":"2022-11-14T23:18:29.711007Z"},"id":"ZXvljgc-jACE","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-15 21:48:59.035878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-11-15 21:49:01.975422: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-11-15 21:49:01.975470: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","2022-11-15 21:49:02.200380: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2022-11-15 21:49:07.388518: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2022-11-15 21:49:07.388781: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2022-11-15 21:49:07.388800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import collections\n","import io\n","import json\n","import os\n","import random\n","import time\n","import urllib\n","import uuid\n","from concurrent.futures import ThreadPoolExecutor\n","from functools import partial\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","from datasets import load_dataset\n","from datasets.utils.file_utils import get_datasets_user_agent\n","from PIL import Image\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"N5TzlNf3dWMX"},"source":["## Set Random Seeds"]},{"cell_type":"markdown","metadata":{"id":"yMZmAIGBhMCy"},"source":["By setting random seed values, you can make your code a bit more reproducible between runs. This will help you as you experiment to see performance boosts due to methodology vs. differences due to initial conditions."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-14T23:18:47.444612Z","iopub.status.busy":"2022-11-14T23:18:47.444234Z","iopub.status.idle":"2022-11-14T23:18:48.667560Z","shell.execute_reply":"2022-11-14T23:18:48.666258Z","shell.execute_reply.started":"2022-11-14T23:18:47.444584Z"},"id":"E0IGjMK_dZIZ","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-15 21:49:16.431762: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-11-15 21:49:16.465581: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2022-11-15 21:49:16.465614: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n","2022-11-15 21:49:16.465639: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (codespaces-2bc9db): /proc/driver/nvidia/version does not exist\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /tmp/ipykernel_12925/2896650489.py:24: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n","\n"]}],"source":["# Seed value\n","seed_value= 1022\n","\n","# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n","import os\n","os.environ['PYTHONHASHSEED']=str(seed_value)\n","\n","# 2. Set the `python` built-in pseudo-random generator at a fixed value\n","import random\n","random.seed(seed_value)\n","\n","# 3. Set the `numpy` pseudo-random generator at a fixed value\n","import numpy as np\n","np.random.seed(seed_value)\n","\n","# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n","import tensorflow as tf\n","tf.random.set_seed(seed_value)\n","\n","# 5. Configure a new global `tensorflow` session\n","from keras import backend as K\n","session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n","tf.compat.v1.keras.backend.set_session(sess)"]},{"cell_type":"markdown","metadata":{"id":"Rc3smN2MjTy9"},"source":["## Data and pre-processing"]},{"cell_type":"markdown","metadata":{"id":"oRO00ulbhlBx"},"source":["Recently, a team at SIL processed the data from the [Bloom Library](https://bloomlibrary.org) in \"AI-ready\" datasets for language modeling, image captioning, speech recognition, and visual storytelling tasks. In particular, the [bloom-captioning](https://huggingface.co/datasets/sil-ai/bloom-captioning) dataset enables developers and researchers to train and/or test image captioning models in 351 languages including Hausa, Kyrgyz, and Thai. This data can be accessed on Hugging Face and/or via the \"datasets\" Python library from Hugging Face.\n","\n","The bloom-captioning dataset includes training, test, and validation splits for many languages, but this competition will only focus on Hausa, Kyrgyz, and Thai. These three languages represent diverse writing system scripts and geographies. The test split of the data does not include the gold standard captions paired with each image. Your job will be to generate the captions for the test split images and upload them to Kaggle. An automatic evaluation will be run on Kaggle to compare your generated captions with the gold standard captions. This evaluation will be used to rank your submission.\n","\n","**Note** - In order to use this dataset, you will need to have a free account on Hugging Face and agree to the terms of use for the dataset. You can do this by:\n","\n","1. Creating your HF account (if you don't yet have one)\n","2. Navigating to your HF account and [create a new \"access token\"](https://huggingface.co/settings/tokens)\n","2. Navigating to the [bloom-captioning](https://huggingface.co/datasets/sil-ai/bloom-captioning) dataset page\n","3. Agreeing to the terms of use at the top of the dataset card\n","4. Downloading and processing the dataset as illustrated below"]},{"cell_type":"markdown","metadata":{"id":"kEdDl1KVjx6K"},"source":["### Fetch the dataset from HF"]},{"cell_type":"markdown","metadata":{"id":"CbGgM1tTiW43"},"source":["Because HF users need to agree to the terms of use of the dataset prior to usage, you need to login to HF as shown below:"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T13:57:40.226858Z","iopub.status.busy":"2022-11-12T13:57:40.225604Z","iopub.status.idle":"2022-11-12T13:57:40.277091Z","shell.execute_reply":"2022-11-12T13:57:40.275958Z","shell.execute_reply.started":"2022-11-12T13:57:40.226802Z"},"id":"dUJlz1PmjEgr","outputId":"ef041b67-2cc8-4359-ae57-7aa81b35d262","trusted":true},"outputs":[],"source":["#! huggingface-cli login\n","#from huggingface_hub import notebook_login\n","#notebook_login()\n","\n","#Here's my HF Read Key for this notebook to use\n","# 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"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T13:57:53.092500Z","iopub.status.busy":"2022-11-12T13:57:53.091599Z","iopub.status.idle":"2022-11-12T13:58:24.640806Z","shell.execute_reply":"2022-11-12T13:58:24.639498Z","shell.execute_reply.started":"2022-11-12T13:57:53.092451Z"},"id":"RocNxtYHjXnr","outputId":"65cf6b14-4133-4059-c347-606ed62e5d93","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading builder script: 100%|██████████| 41.6k/41.6k [00:00<00:00, 724kB/s]\n","Downloading readme: 100%|██████████| 15.2k/15.2k [00:00<00:00, 210kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset bloom-captioning/kir (download: 168.28 MiB, generated: 2.77 MiB, post-processed: Unknown size, total: 171.06 MiB) to /home/codespace/.cache/huggingface/datasets/sil-ai___bloom-captioning/kir/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca...\n"]},{"name":"stderr","output_type":"stream","text":["Downloading data: 100%|██████████| 176M/176M [00:05<00:00, 33.2MB/s] \n","                                                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Dataset bloom-captioning downloaded and prepared to /home/codespace/.cache/huggingface/datasets/sil-ai___bloom-captioning/kir/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca. Subsequent calls will reuse this data.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3/3 [00:00<00:00, 23.40it/s]\n"]}],"source":["# Add the relevant ISO code for the language you want to work with.\n","#iso639_3_letter_code = \"hau\"\n","#iso639_3_letter_code = \"tha\"\n","iso639_3_letter_code = \"kir\"\n","\n","# Download the language specific dataset from HF.\n","access_token = \"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\"\n","dataset = load_dataset(\"sil-ai/bloom-captioning\", iso639_3_letter_code, \n","                       use_auth_token=access_token, download_mode='force_redownload')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T13:58:39.569760Z","iopub.status.busy":"2022-11-12T13:58:39.569363Z","iopub.status.idle":"2022-11-12T13:58:39.579608Z","shell.execute_reply":"2022-11-12T13:58:39.578459Z","shell.execute_reply.started":"2022-11-12T13:58:39.569728Z"},"id":"VA0T9Nb9KjYe","outputId":"9769f1ed-e244-40c8-bb2b-4ac17b06f58c","trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    test: Dataset({\n","        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n","        num_rows: 56\n","    })\n","    validation: Dataset({\n","        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n","        num_rows: 51\n","    })\n","    train: Dataset({\n","        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n","        num_rows: 3919\n","    })\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# See what is included in the dataset object.\n","dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T13:59:06.722415Z","iopub.status.busy":"2022-11-12T13:59:06.721976Z","iopub.status.idle":"2022-11-12T13:59:06.728766Z","shell.execute_reply":"2022-11-12T13:59:06.727681Z","shell.execute_reply.started":"2022-11-12T13:59:06.722366Z"},"id":"6g7n52cGZo7G","outputId":"d57cd026-b028-4033-c584-07420e1cffb5","trusted":true},"outputs":[{"data":{"text/plain":["3919"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Check how many samples (image-caption pairs) are included in our training set.\n","len(dataset['train'])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T13:59:10.093835Z","iopub.status.busy":"2022-11-12T13:59:10.093406Z","iopub.status.idle":"2022-11-12T13:59:10.102487Z","shell.execute_reply":"2022-11-12T13:59:10.101730Z","shell.execute_reply.started":"2022-11-12T13:59:10.093797Z"},"id":"IgIzElWKI97p","outputId":"429afdf2-d387-42db-fdef-6e925d3f78fe","trusted":true},"outputs":[{"data":{"text/plain":["{'image_id': '2888c885-b6ea-485f-bf09-5cbc7d988920',\n"," 'image_url': 'https://bloom-vist.s3.amazonaws.com/%D0%9A%D2%AF%D0%B7%D0%B3%D2%AF/Kuzgu_3_str_Nasynbatova_Svetlana.jpg',\n"," 'caption': '– Кел, бөлүнүп издеп көрөлү. Мен ашканадан издейин. А сен конок бөлмөсүн карачы, – деди Айдай.\\nАдилет:\\n– Туура айтасың. Экөөбүз тең бир жерден издесек, убакыт көп кетет.\\nКонок бөлмөсүндө Адилет телевизордун жанын, дивандын үстүн карады. Ачкыч жок. Килемдин асты менен китеп шкафтын ичин карады. Ал жакта да жок экен.',\n"," 'story_id': '99d18914-ca50-4a80-9d20-8cb510e644a2',\n"," 'album_id': '9278349f-f0cc-4d87-a4b4-0da7992a7552',\n"," 'license': 'cc-by-nc',\n"," 'original_bloom_language_tag': 'ky',\n"," 'index_in_story': 0}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Check one of the training samples.\n","dataset['train'][0]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:00:11.031753Z","iopub.status.busy":"2022-11-12T14:00:11.031303Z","iopub.status.idle":"2022-11-12T14:00:11.039759Z","shell.execute_reply":"2022-11-12T14:00:11.038632Z","shell.execute_reply.started":"2022-11-12T14:00:11.031717Z"},"id":"A_jul7FWJCW5","outputId":"0d432718-2888-4a22-9aa5-11ab1f66989e","trusted":true},"outputs":[{"data":{"text/plain":["{'image_id': '58d4225b-0729-44e6-b364-da182896b90b',\n"," 'image_url': 'https://bloom-vist.s3.amazonaws.com/%D0%A1%D1%83%D1%83+%E2%80%93+%D0%B6%D0%B0%D1%88%D0%BE%D0%BE+%D0%B1%D1%83%D0%BB%D0%B0%D0%B3%D1%8B/The_Time-Travelling_River_3.jpg',\n"," 'caption': '<hidden>',\n"," 'story_id': 'ed11c2fc-ac6e-400d-b568-fb6cc996a3a1',\n"," 'album_id': '0d2513fe-551b-4abe-b8e9-8c9e3f659ca0',\n"," 'license': 'cc-by-nc',\n"," 'original_bloom_language_tag': 'ky',\n"," 'index_in_story': 0}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Check one of the test samples. Notice the hidden caption.\n","dataset['test'][0]"]},{"cell_type":"markdown","metadata":{"id":"jREU72DMjuua"},"source":["### Fetch the images"]},{"cell_type":"markdown","metadata":{"id":"Cy6CMOW1j6dm"},"source":["The actual images are NOT downloaded when you download the Hugging Face dataset. The HF dataset merely includes the public links to the images. As such, you need to download the actual image files. "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:00:41.308159Z","iopub.status.busy":"2022-11-12T14:00:41.307339Z","iopub.status.idle":"2022-11-12T14:03:12.968069Z","shell.execute_reply":"2022-11-12T14:03:12.967027Z","shell.execute_reply.started":"2022-11-12T14:00:41.308118Z"},"id":"F4E1bg5gjkrl","outputId":"8526fca3-3489-4fa6-db4a-a2182439385e","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:02<?, ?ba/s]\n","  0%|          | 0/1 [00:00<?, ?ba/s]"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCanceled future for execute_request message before replies were done"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["! rm -rf images\n","! mkdir images\n","\n","USER_AGENT = get_datasets_user_agent()\n","\n","def fetch_single_image(image_url, timeout=None, retries=0):\n","    request = urllib.request.Request(\n","        image_url,\n","        data=None,\n","        headers={\"user-agent\": USER_AGENT},\n","    )\n","    with urllib.request.urlopen(request, timeout=timeout) as req:\n","        if 'png' in image_url:\n","          png = Image.open(io.BytesIO(req.read())).convert('RGBA')\n","          png.load() # required for png.split()\n","          background = Image.new(\"RGB\", png.size, (255, 255, 255))\n","          background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n","          image_id = str(uuid.uuid4())\n","          image_path = \"images/\" + image_id + \".jpg\"\n","          background.save(image_path, 'JPEG', quality=80)\n","        else:\n","          image = Image.open(io.BytesIO(req.read()))\n","          image_id = str(uuid.uuid4())\n","          image_path = \"images/\" + image_id + \".jpg\"\n","          image.save(image_path)\n","    return image_path\n","\n","def fetch_images(batch, num_threads, timeout=None, retries=3):\n","    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n","    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","        batch[\"image_path\"] = list(executor.map(fetch_single_image_with_args, batch[\"image_url\"]))\n","    return batch\n","\n","num_threads = 20\n","dataset = dataset.map(fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": num_threads})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:03:56.277088Z","iopub.status.busy":"2022-11-12T14:03:56.276694Z","iopub.status.idle":"2022-11-12T14:03:56.285482Z","shell.execute_reply":"2022-11-12T14:03:56.284284Z","shell.execute_reply.started":"2022-11-12T14:03:56.277053Z"},"id":"oKv-iFodIb32","outputId":"0ae15a17-1d17-4692-b94a-cb4d40440c05","trusted":true},"outputs":[],"source":["# Notice that we now have a new field in our dataset object called \"image_path\"\n","dataset['train']"]},{"cell_type":"markdown","metadata":{"id":"6SJP7DCPNYQe"},"source":["## Prepare images features with a pre-trained InceptionV3 model"]},{"cell_type":"markdown","metadata":{"id":"CD3HiQswmpOA"},"source":["This model assumes that the pretrained image encoder is sufficient, and just focuses on building the text decoder. We use the InceptionV3 model for the pretrained image encoder. Read more about InceptionV3 [here](https://keras.io/api/applications/inceptionv3/)."]},{"cell_type":"markdown","metadata":{"id":"ybyKTOfNm3_v"},"source":["### Retrieve the InceptionV3 model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:04:17.405136Z","iopub.status.busy":"2022-11-12T14:04:17.404725Z","iopub.status.idle":"2022-11-12T14:04:25.309854Z","shell.execute_reply":"2022-11-12T14:04:25.308666Z","shell.execute_reply.started":"2022-11-12T14:04:17.405101Z"},"id":"m4ZhUTsxNaGM","outputId":"25a04853-652e-4918-ceec-21c954709590","trusted":true},"outputs":[],"source":["image_model = tf.keras.applications.InceptionV3(include_top=False,\n","                                                weights='imagenet')\n","new_input = image_model.input\n","hidden_layer = image_model.layers[-1].output\n","\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"]},{"cell_type":"markdown","metadata":{"id":"9YALesnjOIVl"},"source":["### Cache image features"]},{"cell_type":"markdown","metadata":{"id":"2tvPGx__nfBy"},"source":["Here we will use the downloaded InceptionV3 model to calculate the encoded features of our images. We will cache these features locally to utilize them during training. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:04:36.749844Z","iopub.status.busy":"2022-11-12T14:04:36.749458Z","iopub.status.idle":"2022-11-12T14:04:36.756205Z","shell.execute_reply":"2022-11-12T14:04:36.754914Z","shell.execute_reply.started":"2022-11-12T14:04:36.749812Z"},"id":"fDrwRbjhOeQd","trusted":true},"outputs":[],"source":["def load_image(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.io.decode_jpeg(img, channels=3)\n","    img = tf.keras.layers.Resizing(299, 299)(img)\n","    img = tf.keras.applications.inception_v3.preprocess_input(img)\n","    return img, image_path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:04:40.505079Z","iopub.status.busy":"2022-11-12T14:04:40.504531Z","iopub.status.idle":"2022-11-12T14:12:10.456807Z","shell.execute_reply":"2022-11-12T14:12:10.455967Z","shell.execute_reply.started":"2022-11-12T14:04:40.505021Z"},"id":"5SsJq0ZcOKWy","outputId":"eaf64a42-2177-4099-ed31-059c16abe327","trusted":true},"outputs":[],"source":["# Get unique train images\n","encode_train = sorted(set(dataset['train']['image_path']))\n","\n","# Feel free to change batch_size according to your system configuration\n","image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","image_dataset = image_dataset.map(\n","  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n","\n","for img, path in tqdm(image_dataset):\n","  batch_features = image_features_extract_model(img)\n","  batch_features = tf.reshape(batch_features,\n","                              (batch_features.shape[0], -1, batch_features.shape[3]))\n","\n","  for bf, p in zip(batch_features, path):\n","    path_of_feature = p.numpy().decode(\"utf-8\")\n","    np.save(path_of_feature, bf.numpy())\n","\n","# Get unique test images\n","encode_test = sorted(set(dataset['test']['image_path']))\n","\n","# Feel free to change batch_size according to your system configuration\n","image_dataset_test = tf.data.Dataset.from_tensor_slices(encode_test)\n","image_dataset_test = image_dataset_test.map(\n","  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n","\n","for img, path in tqdm(image_dataset_test):\n","  batch_features = image_features_extract_model(img)\n","  batch_features = tf.reshape(batch_features,\n","                              (batch_features.shape[0], -1, batch_features.shape[3]))\n","\n","  for bf, p in zip(batch_features, path):\n","    path_of_feature = p.numpy().decode(\"utf-8\")\n","    np.save(path_of_feature, bf.numpy())"]},{"cell_type":"markdown","metadata":{"id":"ChClx3a5Zoi5"},"source":["## Prepare captions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:13:22.292495Z","iopub.status.busy":"2022-11-12T14:13:22.292072Z","iopub.status.idle":"2022-11-12T14:13:22.303560Z","shell.execute_reply":"2022-11-12T14:13:22.302447Z","shell.execute_reply.started":"2022-11-12T14:13:22.292459Z"},"id":"78tFrte9n0li","trusted":true},"outputs":[],"source":["# Add some special tokens and clean up new line characters.\n","train_captions = [f\"<start> {x} <end>\" for x in dataset['train']['caption']]\n","train_captions = [x.replace('\\n', ' ') for x in train_captions]\n","test_captions = [f\"<start> {x} <end>\" for x in dataset['test']['caption']]\n","test_captions = [x.replace('\\n', ' ') for x in test_captions]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:13:25.276652Z","iopub.status.busy":"2022-11-12T14:13:25.276219Z","iopub.status.idle":"2022-11-12T14:13:27.028639Z","shell.execute_reply":"2022-11-12T14:13:27.027495Z","shell.execute_reply.started":"2022-11-12T14:13:25.276610Z"},"id":"JKKFjdCPQVKo","trusted":true},"outputs":[],"source":["caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n","\n","# We will override the default standardization of TextVectorization to preserve\n","# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\n","def standardize(inputs):\n","  inputs = tf.strings.lower(inputs)\n","  return tf.strings.regex_replace(inputs,\n","                                  r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n","\n","# Max word count for a caption.\n","max_length = 50\n","# Use the top 5000 words for a vocabulary.\n","vocabulary_size = 5000\n","tokenizer = tf.keras.layers.TextVectorization(\n","    max_tokens=vocabulary_size,\n","    standardize=standardize,\n","    output_sequence_length=max_length)\n","# Learn the vocabulary from the caption data.\n","tokenizer.adapt(caption_dataset)\n","\n","# Create the tokenized vectors\n","cap_vector = caption_dataset.map(lambda x: tokenizer(x))\n","\n","# Create mappings for words to indices and indicies to words.\n","word_to_index = tf.keras.layers.StringLookup(\n","    mask_token=\"\",\n","    vocabulary=tokenizer.get_vocabulary())\n","index_to_word = tf.keras.layers.StringLookup(\n","    mask_token=\"\",\n","    vocabulary=tokenizer.get_vocabulary(),\n","    invert=True)\n","\n","caption_dataset_test = tf.data.Dataset.from_tensor_slices(test_captions)\n","cap_vector_test = caption_dataset_test.map(lambda x: tokenizer(x))"]},{"cell_type":"markdown","metadata":{"id":"h-Sjl0cvaTuO"},"source":["## More data prep for training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:13:39.784132Z","iopub.status.busy":"2022-11-12T14:13:39.783170Z","iopub.status.idle":"2022-11-12T14:13:40.540763Z","shell.execute_reply":"2022-11-12T14:13:40.539448Z","shell.execute_reply.started":"2022-11-12T14:13:39.784078Z"},"id":"SjhZpyC3aQ64","trusted":true},"outputs":[],"source":["# Create some mas between images, vectors, and captions\n","img_to_cap_vector = collections.defaultdict(list)\n","for img, cap in zip(dataset['train']['image_path'], cap_vector):\n","  img_to_cap_vector[img].append(cap)\n","\n","img_name_train = []\n","cap_train = []\n","for imgt in list(img_to_cap_vector.keys()):\n","  capt_len = len(img_to_cap_vector[imgt])\n","  img_name_train.extend([imgt] * capt_len)\n","  cap_train.extend(img_to_cap_vector[imgt])\n","\n","img_to_cap_vector_test = collections.defaultdict(list)\n","for img, cap in zip(dataset['test']['image_path'], cap_vector_test):\n","  img_to_cap_vector_test[img].append(cap)\n","\n","img_name_test = []\n","cap_test = []\n","for imgv in list(img_to_cap_vector_test.keys()):\n","  capv_len = len(img_to_cap_vector_test[imgv])\n","  img_name_test.extend([imgv] * capv_len)\n","  cap_test.extend(img_to_cap_vector_test[imgv])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:13:44.787036Z","iopub.status.busy":"2022-11-12T14:13:44.786656Z","iopub.status.idle":"2022-11-12T14:13:44.794340Z","shell.execute_reply":"2022-11-12T14:13:44.792979Z","shell.execute_reply.started":"2022-11-12T14:13:44.787006Z"},"id":"OmKOjSSSaUfu","outputId":"89172caf-a9c6-41cf-e1a9-6972c378fc87","trusted":true},"outputs":[],"source":["# Double check the lengths of our new arrays.\n","len(img_name_train), len(cap_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:13:53.260983Z","iopub.status.busy":"2022-11-12T14:13:53.260013Z","iopub.status.idle":"2022-11-12T14:13:53.266672Z","shell.execute_reply":"2022-11-12T14:13:53.265567Z","shell.execute_reply.started":"2022-11-12T14:13:53.260944Z"},"id":"_V7xQdp2aY5r","trusted":true},"outputs":[],"source":["# Feel free to change these parameters according to your system's configuration\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","embedding_dim = 256\n","units = 512\n","num_steps = len(img_name_train) // BATCH_SIZE\n","\n","# Shape of the vector extracted from InceptionV3 is (64, 2048)\n","# These two variables represent that vector shape\n","features_shape = 2048\n","attention_features_shape = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:13:57.844629Z","iopub.status.busy":"2022-11-12T14:13:57.844245Z","iopub.status.idle":"2022-11-12T14:13:57.848880Z","shell.execute_reply":"2022-11-12T14:13:57.848052Z","shell.execute_reply.started":"2022-11-12T14:13:57.844597Z"},"id":"hYFY2qUyarMB","trusted":true},"outputs":[],"source":["# Load the numpy files\n","def map_func(img_name, cap):\n","  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n","  return img_tensor, cap"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:14:01.022744Z","iopub.status.busy":"2022-11-12T14:14:01.022348Z","iopub.status.idle":"2022-11-12T14:14:01.099439Z","shell.execute_reply":"2022-11-12T14:14:01.098588Z","shell.execute_reply.started":"2022-11-12T14:14:01.022712Z"},"id":"YJe-wn8xasdd","trusted":true},"outputs":[],"source":["dataset_tf = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","\n","# Use map to load the numpy files in parallel\n","dataset_tf = dataset_tf.map(lambda item1, item2: tf.numpy_function(\n","          map_func, [item1, item2], [tf.float32, tf.int64]),\n","          num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# Shuffle and batch\n","dataset_tf = dataset_tf.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset_tf = dataset_tf.prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"uQOfL_jsawMp"},"source":["## Define our image captioning model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:14:08.480597Z","iopub.status.busy":"2022-11-12T14:14:08.479738Z","iopub.status.idle":"2022-11-12T14:14:08.488061Z","shell.execute_reply":"2022-11-12T14:14:08.487119Z","shell.execute_reply.started":"2022-11-12T14:14:08.480563Z"},"id":"EmIemwgNatoq","trusted":true},"outputs":[],"source":["class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, features, hidden):\n","    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n","\n","    # hidden shape == (batch_size, hidden_size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n","    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","\n","    # attention_hidden_layer shape == (batch_size, 64, units)\n","    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n","                                         self.W2(hidden_with_time_axis)))\n","\n","    # score shape == (batch_size, 64, 1)\n","    # This gives you an unnormalized score for each image feature.\n","    score = self.V(attention_hidden_layer)\n","\n","    # attention_weights shape == (batch_size, 64, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * features\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:14:12.483703Z","iopub.status.busy":"2022-11-12T14:14:12.483309Z","iopub.status.idle":"2022-11-12T14:14:12.491729Z","shell.execute_reply":"2022-11-12T14:14:12.490376Z","shell.execute_reply.started":"2022-11-12T14:14:12.483671Z"},"id":"GyxXsF3_axFb","trusted":true},"outputs":[],"source":["class CNN_Encoder(tf.keras.Model):\n","    # Since you have already extracted the features and dumped it\n","    # This encoder passes those features through a Fully connected layer\n","    def __init__(self, embedding_dim):\n","        super(CNN_Encoder, self).__init__()\n","        # shape after fc == (batch_size, 64, embedding_dim)\n","        self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","    def call(self, x):\n","        x = self.fc(x)\n","        x = tf.nn.relu(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:14:16.005732Z","iopub.status.busy":"2022-11-12T14:14:16.004996Z","iopub.status.idle":"2022-11-12T14:14:16.015621Z","shell.execute_reply":"2022-11-12T14:14:16.014611Z","shell.execute_reply.started":"2022-11-12T14:14:16.005696Z"},"id":"88T53JesazIr","trusted":true},"outputs":[],"source":["class RNN_Decoder(tf.keras.Model):\n","  def __init__(self, embedding_dim, units, vocab_size):\n","    super(RNN_Decoder, self).__init__()\n","    self.units = units\n","\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc1 = tf.keras.layers.Dense(self.units)\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","    self.attention = BahdanauAttention(self.units)\n","\n","  def call(self, x, features, hidden):\n","    # defining attention as a separate model\n","    context_vector, attention_weights = self.attention(features, hidden)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","\n","    # shape == (batch_size, max_length, hidden_size)\n","    x = self.fc1(output)\n","\n","    # x shape == (batch_size * max_length, hidden_size)\n","    x = tf.reshape(x, (-1, x.shape[2]))\n","\n","    # output shape == (batch_size * max_length, vocab)\n","    x = self.fc2(x)\n","\n","    return x, state, attention_weights\n","\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-12T14:14:20.676802Z","iopub.status.busy":"2022-11-12T14:14:20.676146Z","iopub.status.idle":"2022-11-12T14:14:20.696221Z","shell.execute_reply":"2022-11-12T14:14:20.695079Z","shell.execute_reply.started":"2022-11-12T14:14:20.676768Z"},"id":"5wKm0qbPa0wq","trusted":true},"outputs":[],"source":["# Initialize the encoder and decoder\n","encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-L6W_p9a2Kx"},"outputs":[],"source":["# Training config.\n","optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","# Loss function to use during training.\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"]},{"cell_type":"markdown","metadata":{"id":"ZHqzia1ea37r"},"source":["## Setup and execute training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ggMkCTKEa3Z9"},"outputs":[],"source":["# Make sure we save checkpoints during training\n","checkpoint_path = \"./checkpoints/train\"\n","ckpt = tf.train.Checkpoint(encoder=encoder,\n","                           decoder=decoder,\n","                           optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRatak4Na6gd"},"outputs":[],"source":["start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","  \n","  # restoring the latest checkpoint in checkpoint_path\n","  ckpt.restore(ckpt_manager.latest_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0828wbXa8d_"},"outputs":[],"source":["# adding this in a separate cell because if you run the training cell\n","# many times, the loss_plot array will be reset\n","loss_plot = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPd14RVGa_IJ"},"outputs":[],"source":["@tf.function\n","def train_step(img_tensor, target):\n","  loss = 0\n","\n","  # initializing the hidden state for each batch\n","  # because the captions are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n","\n","  with tf.GradientTape() as tape:\n","      features = encoder(img_tensor)\n","\n","      for i in range(1, target.shape[1]):\n","          # passing the features through the decoder\n","          predictions, hidden, _ = decoder(dec_input, features, hidden)\n","\n","          loss += loss_function(target[:, i], predictions)\n","\n","          # using teacher forcing\n","          dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(loss, trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","  return loss, total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yw_aGJpsbBdp","outputId":"bf095fd3-bf17-43ff-d89b-7a4262e5b901"},"outputs":[],"source":["# Adjust this depending on how long you want to train\n","EPOCHS = 5\n","\n","# Train our model!\n","for epoch in range(start_epoch, EPOCHS):\n","    start = time.time()\n","    total_loss = 0\n","\n","    for (batch, (img_tensor, target)) in enumerate(dataset_tf):\n","        batch_loss, t_loss = train_step(img_tensor, target)\n","        total_loss += t_loss\n","\n","        if batch % 100 == 0:\n","            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n","            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n","    # storing the epoch end loss value to plot later\n","    loss_plot.append(total_loss / num_steps)\n","\n","    if epoch % 5 == 0:\n","      ckpt_manager.save()\n","\n","    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n","    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8zS5bsUbDBC","outputId":"7a89c64f-bf0c-4602-dcf6-120da4eac519"},"outputs":[],"source":["# Visualize our loss\n","plt.plot(loss_plot)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss Plot')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-1a3AwSMMsaM"},"source":["## Predict a caption for the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQcF5FuPMvHf"},"outputs":[],"source":["def predict(image):\n","    attention_plot = np.zeros((max_length, attention_features_shape))\n","\n","    hidden = decoder.reset_state(batch_size=1)\n","\n","    temp_input = tf.expand_dims(load_image(image)[0], 0)\n","    img_tensor_val = image_features_extract_model(temp_input)\n","    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n","                                                 -1,\n","                                                 img_tensor_val.shape[3]))\n","\n","    features = encoder(img_tensor_val)\n","\n","    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n","    result = []\n","\n","    for i in range(max_length):\n","        predictions, hidden, attention_weights = decoder(dec_input,\n","                                                         features,\n","                                                         hidden)\n","\n","        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","\n","        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","        predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n","        result.append(predicted_word)\n","\n","        if predicted_word == '<end>':\n","            return result, attention_plot\n","\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    attention_plot = attention_plot[:len(result), :]\n","    return result, attention_plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6VGYuJ-oM7iP"},"outputs":[],"source":["def plot_attention(image, result, attention_plot):\n","    temp_image = np.array(Image.open(image))\n","\n","    fig = plt.figure(figsize=(10, 10))\n","\n","    len_result = len(result)\n","    for i in range(len_result):\n","        temp_att = np.resize(attention_plot[i], (8, 8))\n","        grid_size = max(int(np.ceil(len_result/2)), 2)\n","        ax = fig.add_subplot(grid_size, grid_size, i+1)\n","        ax.set_title(result[i])\n","        img = ax.imshow(temp_image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_kNYfd1M-mx","outputId":"cfe8d738-a4bb-490c-8e78-7838d748e3e1"},"outputs":[],"source":["# Predict a caption for a random test image\n","rid = np.random.randint(0, len(img_name_test))\n","image = img_name_test[rid]\n","result, attention_plot = predict(image)\n","predicted_caption = ' '.join(result).replace(' <end>', '')\n","print('Prediction Caption:', predicted_caption)\n","\n","# Display image\n","from IPython.display import Image as im\n","im(filename=image) "]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"}}},"nbformat":4,"nbformat_minor":4}
